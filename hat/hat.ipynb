{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-02T17:33:18.986334854Z",
     "start_time": "2023-06-02T17:33:18.943121750Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "train_raw = pd.read_json('data/en_ewt-up-train.jsonl',lines=True)\n",
    "dev_raw = pd.read_json('data/en_ewt-up-dev.jsonl',lines=True)\n",
    "test_raw = pd.read_json('data/en_ewt-up-test.jsonl',lines=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T17:33:20.012847046Z",
     "start_time": "2023-06-02T17:33:18.986503192Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                    seq_words   \n",
      "0      [Al, -, Zaman, :, American, forces, killed, Shaikh, Abdullah, al, -, Ani, ,, the, preacher, at, the, mosque, in, the, town, of, Qaim, ,, near, the, Syrian, border, .]  \\\n",
      "1                                                                    [[, This, killing, of, a, respected, cleric, will, be, causing, us, trouble, for, years, to, come, ., ]]   \n",
      "2                                                                    [[, This, killing, of, a, respected, cleric, will, be, causing, us, trouble, for, years, to, come, ., ]]   \n",
      "3                                                                    [[, This, killing, of, a, respected, cleric, will, be, causing, us, trouble, for, years, to, come, ., ]]   \n",
      "4                                                                    [[, This, killing, of, a, respected, cleric, will, be, causing, us, trouble, for, years, to, come, ., ]]   \n",
      "...                                                                                                                                                                       ...   \n",
      "42466           [I, will, never, return, there, again, (, and, now, have, some, serious, doubts, about, the, quality, of, work, they, actually, performed, on, my, car, ), .]   \n",
      "42467           [I, will, never, return, there, again, (, and, now, have, some, serious, doubts, about, the, quality, of, work, they, actually, performed, on, my, car, ), .]   \n",
      "42468           [I, will, never, return, there, again, (, and, now, have, some, serious, doubts, about, the, quality, of, work, they, actually, performed, on, my, car, ), .]   \n",
      "42469           [I, will, never, return, there, again, (, and, now, have, some, serious, doubts, about, the, quality, of, work, they, actually, performed, on, my, car, ), .]   \n",
      "42470           [I, will, never, return, there, again, (, and, now, have, some, serious, doubts, about, the, quality, of, work, they, actually, performed, on, my, car, ), .]   \n",
      "\n",
      "                                                                                                                         BIO   \n",
      "0               [O, O, O, O, O, B-ARG0, B-V, B-ARG1, O, O, O, O, O, O, O, O, O, B-ARGM-LOC, O, O, O, O, O, O, O, O, O, O, O]  \\\n",
      "1                                                              [O, O, B-V, O, O, O, B-ARG1, O, O, O, O, O, O, O, O, O, O, O]   \n",
      "2                                                                   [O, O, O, O, O, O, O, O, B-V, O, O, O, O, O, O, O, O, O]   \n",
      "3                              [O, O, B-ARG0, O, O, O, O, B-ARGM-MOD, O, B-V, B-ARGM-GOL, B-ARG1, O, B-ARGM-TMP, O, O, O, O]   \n",
      "4                                                              [O, O, O, O, O, O, O, O, O, O, O, O, O, B-ARG1, O, B-V, O, O]   \n",
      "...                                                                                                                      ...   \n",
      "42466  [B-ARG1, B-ARGM-MOD, B-ARGM-NEG, B-V, B-ARG4, B-ARGM-TMP, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
      "42467                              [O, O, O, O, O, O, O, O, O, B-V, O, O, B-ARGM-PRR, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
      "42468  [B-ARG0, O, O, O, O, O, O, O, B-ARGM-TMP, B-ARGM-LVB, O, B-ARGM-ADJ, B-V, O, O, B-ARG1, O, O, O, O, O, O, O, O, O, O]   \n",
      "42469           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-V, B-ARG0, B-ARGM-ADV, B-ARGM-LVB, O, O, B-ARG1, O, O]   \n",
      "42470                              [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ARGM-PRR, O, O, B-V, O, O, O, O, O]   \n",
      "\n",
      "                             pred_sense src_lang  \n",
      "0             [6, killed, kill.01, VBD]     <EN>  \n",
      "1             [2, killing, kill.01, NN]     <EN>  \n",
      "2                    [8, be, be.03, VB]     <EN>  \n",
      "3           [9, causing, cause.01, VBG]     <EN>  \n",
      "4               [15, come, come.01, VB]     <EN>  \n",
      "...                                 ...      ...  \n",
      "42466        [3, return, return.01, VB]     <EN>  \n",
      "42467           [9, have, have.LV, VBP]     <EN>  \n",
      "42468       [12, doubts, doubt.01, NNS]     <EN>  \n",
      "42469           [17, work, work.01, NN]     <EN>  \n",
      "42470  [20, performed, perform.LV, VBD]     <EN>  \n",
      "\n",
      "[42471 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_raw)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T17:33:20.022092978Z",
     "start_time": "2023-06-02T17:33:20.012189285Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The silly token_classification preproc code requires a space separated list of ints, with one per token including punctuation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T17:33:20.022274936Z",
     "start_time": "2023-06-02T17:33:20.014724550Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450                                                                                                                                                                      [O]\n",
      "451                                                                                                                                                                      [O]\n",
      "452                                                                                                                                                                      [O]\n",
      "453                                                                                                                                                                      [O]\n",
      "454                                                                                                                                                                      [O]\n",
      "455                                                                                                                                                                      [O]\n",
      "456                                                                                                                                                                      [O]\n",
      "457                                                                                                                                                                      [O]\n",
      "458                                                                                                                                                                      [O]\n",
      "459                                                                                                                                                                      [O]\n",
      "460                                                                                                                                                                      [O]\n",
      "461                                                                                           [B-ARG1, B-ARGM-MOD, B-V, B-ARG2, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]\n",
      "462                                                                                           [B-ARG1, B-ARGM-MOD, O, B-V, O, B-ARG2, O, O, O, O, O, O, O, O, O, O, O, O, O]\n",
      "463                                                                                                    [B-ARG0, O, O, O, O, B-V, O, B-ARG1, O, O, O, O, O, O, O, O, O, O, O]\n",
      "464                                                                                                              [O, O, O, O, O, O, O, B-V, O, O, O, O, O, O, O, O, O, O, O]\n",
      "465                                                                                                    [O, O, O, O, O, O, O, B-ARG1, B-ARG0, B-V, O, O, O, O, O, O, O, O, O]\n",
      "466                                                                                                    [O, O, O, O, O, O, O, O, O, O, O, B-ARG1, B-ARG0, B-V, O, O, O, O, O]\n",
      "467                                                                                                    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ARG1, B-V, O, B-ARG2, O]\n",
      "468                                                                                                [O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ARG0, O, B-ARGM-EXT, B-V, O]\n",
      "469                                                                                                                                                                 [B-V, O]\n",
      "470    [O, O, B-ARGM-PRD, O, O, O, B-ARG0, B-V, O, B-ARG1, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]\n",
      "471        [O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ARG0, B-V, B-ARG2, O, O, B-ARG3, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]\n",
      "472         [O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ARG0, O, O, O, O, O, O, O, O, O, O, O, O, B-V, O, O, O, O, O, O, B-ARGM-TMP, O, O, O, O, O, O, O, O, O, O, O, O, O]\n",
      "473           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ARG1, O, O, O, B-V, B-C-ARG1, O, O, O, O, O, O, O, O, O, O, O, O]\n",
      "474         [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ARGM-TMP, O, B-ARG1, O, O, O, O, B-V, O, O, O, O, O, O, O, O, O, O, O, O]\n",
      "475             [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ARG0, B-V, O, O, O, B-ARG1, O, O, O, O, O]\n",
      "476    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ARG1, B-V, B-ARGM-NEG, B-ARG2, O, O, O, O, O]\n",
      "477             [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ARG0, O, O, O, O, O, O, B-V, O, B-ARG1, O]\n",
      "478                       [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-V, O]\n",
      "479                                                                                                                        [B-V, O, O, O, O, B-ARG1, O, O, O, O, O, O, O, O]\n",
      "480                                                                                                                        [O, O, O, B-ARG1, B-V, O, O, O, O, O, O, O, O, O]\n",
      "481                                                                                                            [O, O, O, O, O, B-ARG1, B-R-ARG1, B-V, O, B-ARG2, O, O, O, O]\n",
      "482                                                                                                                    [O, O, O, O, O, O, O, O, O, O, O, B-ARGM-PRP, B-V, O]\n",
      "483                                                                                                                                                        [O, O, B-V, O, O]\n",
      "484                                                                                                                                          [B-ARG1, B-ARGM-DIS, O, B-V, O]\n",
      "485                                                                       [B-V, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]\n",
      "486                                                                       [O, O, B-V, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]\n",
      "487                                                                  [O, O, O, O, B-V, O, O, O, B-ARG1, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]\n",
      "488                                                                       [O, O, O, O, O, O, B-V, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]\n",
      "489                                                                       [O, O, O, O, O, O, O, B-V, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]\n",
      "490                                                        [O, O, O, O, O, B-ARG1, B-ARG0, O, B-V, O, O, O, O, O, O, O, O, B-ARG2, O, O, O, O, O, O, O, O, O, O, O, O, O, O]\n",
      "491                                                                       [O, O, O, O, O, O, O, O, O, O, B-V, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]\n",
      "492                                               [O, O, O, O, O, B-ARG1, B-ARG0, O, O, O, O, B-ARGM-TMP, B-V, O, O, O, O, B-ARG2, O, O, O, O, O, O, O, O, O, O, O, O, O, O]\n",
      "493                                                         [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ARGM-MOD, B-V, O, B-ARG1, O, O, O, O, O, O, O, O]\n",
      "494                                                         [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ARGM-NEG, B-V, O, O, B-ARG1, O, O, O, O, O]\n",
      "495                                                                  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-V, O, B-ARG0, O, O, O]\n",
      "496                                                              [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-V, O, B-ARGM-LOC, O]\n",
      "497                                                                                                    [O, O, O, O, B-ARG0, O, O, O, B-V, O, B-ARG1, O, O, O, O, O, O, O, O]\n",
      "498                                                                                                [O, O, O, O, O, O, O, O, O, B-ARGM-ADJ, B-V, O, B-ARG1, O, O, O, O, O, O]\n",
      "499                                                                                                [O, O, O, O, O, O, O, O, O, O, B-ARGM-ADV, O, B-V, O, B-ARG2, O, O, O, O]\n",
      "Name: BIO, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "print(train_raw['BIO'].head(500).tail(50))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T17:33:20.031500438Z",
     "start_time": "2023-06-02T17:33:20.016367337Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B-ARGA', 1), ('B-ARG5', 4), ('B-ARGM-REC', 24), ('B-ARG1-DSP', 47), ('B-ARGM-COM', 114), ('B-ARGM-CXN', 200), ('B-ARGM-GOL', 227), ('B-ARG4', 414), ('B-ARGM-PRD', 459), ('B-ARGM-DIR', 500), ('B-ARGM-LVB', 553), ('B-ARGM-PRR', 556), ('B-ARGM-CAU', 607), ('B-ARG3', 682), ('B-ARGM-PRP', 721), ('B-ARGM-EXT', 875), ('B-ARGM-DIS', 1426), ('B-ARGM-MNR', 1501), ('B-ARGM-ADJ', 1691), ('B-ARGM-LOC', 1697), ('B-ARGM-NEG', 1909), ('B-ARGM-MOD', 3570), ('B-ARGM-ADV', 4462), ('B-ARGM-TMP', 5374), ('B-ARG2', 9959), ('B-ARG0', 15953), ('B-ARG1', 28772), ('B-V', 40661), ('O', 912945)]\n"
     ]
    }
   ],
   "source": [
    "letcount = {}\n",
    "for y in train_raw['BIO']:\n",
    "    for xx in y:\n",
    "        x = xx.replace(\"-C-\",\"-\").replace(\"-R-\",\"-\")\n",
    "        if x not in letcount:\n",
    "            letcount[x] = 1\n",
    "        else:\n",
    "            letcount[x] +=1\n",
    "qq = (sorted(letcount.items(), key=lambda q: q[1]))\n",
    "print(qq)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T17:33:20.179648431Z",
     "start_time": "2023-06-02T17:33:20.025766227Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-ARG3': 3, 'B-ARGM-MOD': 8, 'B-ARGM-ADV': 7, 'B-ARGM-TMP': 6, 'B-ARG2': 2, 'B-ARG0': 0, 'B-ARG1': 1, 'B-V': 4, 'O': 5} {3: 'B-ARG3', 8: 'B-ARGM-MOD', 7: 'B-ARGM-ADV', 6: 'B-ARGM-TMP', 2: 'B-ARG2', 0: 'B-ARG0', 1: 'B-ARG1', 4: 'B-V', 5: 'O'}\n"
     ]
    }
   ],
   "source": [
    "label_map = {'B-ARG3': 3, 'B-ARGM-MOD': 8, 'B-ARGM-ADV': 7, 'B-ARGM-TMP': 6, 'B-ARG2': 2, 'B-ARG0': 0, 'B-ARG1':1, 'B-V': 4, 'O':5}\n",
    "\n",
    "label_map_rev = {v: k for k,v in label_map.items()}\n",
    "\n",
    "def label2id(label):\n",
    "    label = label.replace(\"-C-\",\"-\").replace(\"-R-\",\"-\")\n",
    "    return label_map.get(label, 5)\n",
    "\n",
    "def check_lens(row):\n",
    "    assert len(row['seq_words']) == len(row['labels'])\n",
    "    return True\n",
    "\n",
    "\n",
    "print(label_map, label_map_rev)\n",
    "train_raw['labels'] = train_raw['BIO'].apply(lambda q: list(map(label2id, q)))\n",
    "train_raw['checked'] = train_raw.apply(check_lens, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T17:33:20.444679028Z",
     "start_time": "2023-06-02T17:33:20.183097513Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wnut_17 (/home/titan-0/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "907864b3536e4e7b9cad83198fe95d22"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wnut = load_dataset(\"wnut_17\")\n",
    "example = wnut[\"train\"][0]\n",
    "print(example)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T17:37:43.894568170Z",
     "start_time": "2023-06-02T17:37:42.850840240Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-100, 5, 5, 5, -100, 5, 5, 0, 4, 1, -100, 5, 5, 5, 5, -100, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100, -100, 5, 5, 5, 5, 5, 5, -100], [-100, 5, 5, 4, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100], [-100, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100], [-100, 5, 5, 0, 5, 5, 5, 5, 8, 5, 4, 5, 1, 5, 6, 5, 5, 5, 5, -100], [-100, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 4, 5, 5, -100], [-100, 5, -100, 5, 5, 0, 4, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, -100], [-100, 5, -100, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100], [-100, 5, -100, 5, 5, 5, 5, 5, 0, 5, 4, 5, 5, 5, 1, 5, 5, 5, 5, -100], [-100, 5, -100, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 4, 5, 5, 5, -100], [-100, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100], [-100, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100], [-100, 1, 5, 5, 5, 5, 4, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, -100], [-100, 5, 1, 5, 5, 4, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100, 5, 5, 5, -100, -100, -100, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100, 5, 5, 5, 5, 5, 5, -100], [-100, 5, 1, 5, 5, 5, 4, 5, 5, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, -100, 5, 5, 5, -100, -100, -100, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100, 5, 5, 5, 5, 5, 5, -100], [-100, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 1, 8, 4, 5, 2, 5, -100, 5, 5, 5, -100, -100, -100, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100, 5, 5, 5, 5, 5, 5, -100], [-100, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, -100, 5, 5, 5, -100, -100, -100, 1, 5, 5, 5, 5, 5, 5, 5, 5, -100, 5, 5, 5, 5, 5, 5, -100], [-100, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, -100, 5, 5, 5, -100, -100, -100, 4, 5, 5, 5, 7, 1, 5, 5, 5, -100, 5, 6, 5, 5, 6, 5, -100], [-100, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100], [-100, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100], [-100, 5, 1, 5, 5, 4, 5, 5, 0, 5, 5, 5, 5, 5, -100], [-100, 0, 4, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, -100], [-100, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, -100], [-100, 5, 5, 5, 0, 5, 4, 5, 1, 5, 5, 5, 5, 5, -100], [-100, 5, 5, 4, 5, 5, 5, 5, -100, -100, -100, -100, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100], [-100, 5, 0, 5, 4, 5, 5, 5, -100, -100, -100, -100, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100], [-100, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100, -100, 5, 5, 5, 5, 5, 5, -100, -100, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100], [-100, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100, -100, 5, 5, 5, 5, 5, 5, -100, -100, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100], [-100, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100, -100, 5, 5, 5, 5, 5, 5, -100, -100, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100], [-100, 5, 5, 0, 5, 5, 5, 5, 4, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100, -100, 5, 5, 5, 5, 5, 5, -100, -100, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100], [-100, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100, -100, 5, 5, 5, 5, 5, 5, -100, -100, 5, 5, 5, 5, 5, 5, 5, 5, 5, -100]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 36 at dim 1 (got 20)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[81], line 41\u001B[0m\n\u001B[1;32m     38\u001B[0m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtokens\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m train_raw[:\u001B[38;5;241m30\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mseq_words\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m     39\u001B[0m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m train_raw[:\u001B[38;5;241m30\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[0;32m---> 41\u001B[0m inputs \u001B[38;5;241m=\u001B[39m \u001B[43mtokenize_and_align_labels\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28mprint\u001B[39m(inputs)\n\u001B[1;32m     46\u001B[0m \u001B[38;5;66;03m# Training loop\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[81], line 32\u001B[0m, in \u001B[0;36mtokenize_and_align_labels\u001B[0;34m(examples)\u001B[0m\n\u001B[1;32m     29\u001B[0m     labels\u001B[38;5;241m.\u001B[39mappend(label_ids)\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28mprint\u001B[39m(labels)\n\u001B[0;32m---> 32\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenized_inputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     33\u001B[0m tokenized_inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(labels)\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tokenized_inputs\n",
      "\u001B[0;31mValueError\u001B[0m: expected sequence of length 36 at dim 1 (got 20)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "\n",
    "#each row is a dict, with a tokens/seq_words entry, which is a list of words, and a labels entry, which is a list of ints.\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_map))\n",
    "model = model.to('cuda:0')\n",
    "# Preprocess data and tokenize input texts\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    print(labels)\n",
    "    print(torch.tensor(tokenized_inputs['input_ids']))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = torch.tensor(labels)\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "data = {}\n",
    "data['tokens'] = train_raw[:30]['seq_words'].tolist()\n",
    "data['labels'] = train_raw[:30]['labels'].tolist()\n",
    "\n",
    "inputs = tokenize_and_align_labels(data)\n",
    "\n",
    "print(inputs)\n",
    "\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "data\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(data_collator)\n",
    "    loss = outputs.loss\n",
    "    logits = outputs.logits\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "#model.save_pretrained('hf_trained_srl')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T17:45:40.700153853Z",
     "start_time": "2023-06-02T17:45:39.879904502Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
